# Laboratorios Introductorios de PySpark en Google Colab

Este repositorio contiene cuatro laboratorios prácticos diseñados para aprender PySpark desde cero, comprender cómo trabaja con Big Data y aplicar modelos de aprendizaje automático distribuidos.
Todo está optimizado para ejecutarse fácilmente en Google Colab, sin necesidad de instalaciones locales.

## 1. Introducción a PySpark con Google Colab

Objetivo:
Aprender a configurar el entorno de PySpark en Google Colab y ejecutar operaciones básicas de análisis de datos distribuidos.


Contenido

* Instalación y configuración de PySpark.

* Creación de una sesión de Spark (SparkSession).

* Exploración y manipulación básica de datos

Aprendizaje clave:
Comprender cómo PySpark gestiona los datos de manera paralela y escalable, a diferencia de Python tradicional.


## 2. Análisis de las 5V del Big Data con PySpark
  
Objetivo:
Explorar las 5 dimensiones del Big Data (Volumen, Velocidad, Variedad, Veracidad y Valor) utilizando un dataset educativo y las funciones de PySpark.


Contenido

* Volumen: Conteo de registros, columnas y datos nulos.
* Velocidad: Medición del tiempo de ejecución de operaciones distribuidas.
* Variedad: Identificación de tipos de datos y estructuras.
* Veracidad: Limpieza, eliminación de duplicados y estandarización de valores.
* Valor: Obtención de insights a partir de agrupaciones e indicadores estadísticos.

Aprendizaje clave:
Evaluar la calidad y el potencial analítico de un dataset masivo aplicando las 5V con PySpark.


## 3. Conexión a Apache Spark y trabajo con RDDs

Objetivo:
Comprender cómo funciona Apache Spark para el procesamiento distribuido de datos mediante la creación, transformación y análisis de RDDs (Resilient Distributed Datasets) en un entorno de práctica con Google Colab.

Componentes Analizados

* Configuración del entorno PySpark.
* Creación y exploración de RDDs a partir de un DataFrame.
* Aplicación de transformaciones (map, filter, reduceByKey, sortBy).
* Ejecución de acciones (take, count, collect).
* Simulación del procesamiento distribuido y almacenamiento en caché.

Aprendizaje Clave:
Entender del modelo de ejecución distribuido de Spark.


## 4. Codificación y transformación de variables

Objetivo:
Desarrollar habilidades fundamentales para preparar datos antes de entrenar modelos de inteligencia artificial.

Contenido

* Identificación de variables categóricas y numéricas
* Codificación de variables categóricas
* Transformación de variables numéricas

Aprendizaje Clave:
Comprender la importancia del preprocesamiento en flujos de IA.
