# Laboratorios Introductorios de PySpark en Google Colab

Este repositorio contiene cuatro laboratorios prácticos diseñados para aprender PySpark desde cero, comprender cómo trabaja con Big Data y aplicar modelos de aprendizaje automático distribuidos.
Todo está optimizado para ejecutarse fácilmente en Google Colab, sin necesidad de instalaciones locales.

1. Introducción a PySpark con Google Colab

Objetivo
Aprender a configurar el entorno de PySpark en Google Colab y ejecutar operaciones básicas de análisis de datos distribuidos.


Contenido

* Instalación y configuración de PySpark.

* Creación de una sesión de Spark (SparkSession).

* Exploración y manipulación básica de datos

Aprendizaje clave
Comprender cómo PySpark gestiona los datos de manera paralela y escalable, a diferencia de Python tradicional.


2. Análisis de las 5V del Big Data con PySpark
  
Objetivo
Explorar las 5 dimensiones del Big Data (Volumen, Velocidad, Variedad, Veracidad y Valor) utilizando un dataset educativo y las funciones de PySpark.


Componentes Analizados

* Volumen: Conteo de registros, columnas y datos nulos.
* Velocidad: Medición del tiempo de ejecución de operaciones distribuidas.
* Variedad: Identificación de tipos de datos y estructuras.
* Veracidad: Limpieza, eliminación de duplicados y estandarización de valores.
* Valor: Obtención de insights a partir de agrupaciones e indicadores estadísticos.

Aprendizaje clave
Evaluar la calidad y el potencial analítico de un dataset masivo aplicando las 5V con PySpark.
