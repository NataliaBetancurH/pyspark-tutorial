{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Autor: Natalia Betancur Herrera\n",
        "\n",
        "Fecha: 6/10/2025"
      ],
      "metadata": {
        "id": "Fshn9ERYJ0vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio Análisis de las 5V del Big Data con PySpark en Google Colab\n",
        "\n",
        "\n",
        "##Introducción\n",
        "\n",
        "En este laboratorio exploraremos cómo Apache Spark, a través de su interfaz PySpark, nos permite analizar grandes volúmenes de datos de manera distribuida, eficiente y escalable. Trabajaremos dentro de Google Colab, un entorno gratuito y basado en la nube que facilita la ejecución de código en Python sin necesidad de instalaciones locales, aprovechando los recursos computacionales de Google.\n",
        "\n",
        "El objetivo principal de este laboratorio es comprender y aplicar los conceptos de las 5V del Big Data (Volumen, Velocidad, Variedad, Veracidad y Valor) sobre un conjunto de datos real. Cada V representa una característica esencial que define los retos y oportunidades del procesamiento de grandes volúmenes de información:\n",
        "\n",
        "Durante el desarrollo del laboratorio, aprenderás a:\n",
        "\n",
        "* Configurar un entorno de PySpark en Google Colab.\n",
        "\n",
        "* Cargar y explorar un dataset.\n",
        "\n",
        "* Aplicar funciones distribuidas de transformación, limpieza y análisis.\n",
        "\n",
        "* Evaluar las 5V del Big Data utilizando código y métricas interpretativas.\n",
        "\n",
        "Al finalizar, habrás adquirido una comprensión práctica de cómo PySpark transforma el análisis de datos a gran escala, permitiéndote abordar problemas complejos en educación, salud, industria o sostenibilidad con herramientas de procesamiento masivo.\n",
        "\n"
      ],
      "metadata": {
        "id": "1mzmSY2NKBnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalación y configuración del entorno"
      ],
      "metadata": {
        "id": "TZmc-VOjTHO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark --quiet"
      ],
      "metadata": {
        "id": "bDuJvbaqTLlZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5UD844n3TI5X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Analisis_5V_BigData\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ZJHPbK71TOZZ",
        "outputId": "3c388645-968e-4b28-8f27-1347f7aedbe6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6ef6c65700>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0f8b9db4944f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Analisis_5V_BigData</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Cargar el dataset"
      ],
      "metadata": {
        "id": "czlk2jzjTQ3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonar el repositorio\n",
        "!git clone https://github.com/NataliaBetancurH/pyspark-tutorial.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XLy9aM4TSfA",
        "outputId": "c91dc8bf-c98a-4e3b-94e4-3a55ff576d12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pyspark-tutorial' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "df = spark.read.csv(\"/content/pyspark-tutorial/dataset_educativo.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "KYtMgf_VV7pZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar tipo y estructura\n",
        "print(\" Entorno PySpark listo\")\n",
        "print(f\"Tipo de objeto df: {type(df)}\")\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEkJGWjEUQrl",
        "outputId": "8003ae2c-69af-48eb-cef5-81dbf2133f90"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Entorno PySpark listo\n",
            "Tipo de objeto df: <class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+---+---------------+---------+------------+\n",
            "| ID|   Departamento|   Genero|       Grado|\n",
            "+---+---------------+---------+------------+\n",
            "|  1|      Santander|Masculino|    Primaria|\n",
            "|  2|      Santander|        F|  Secundaria|\n",
            "|  3|Valle del Cauca| Femenino|Bachillerato|\n",
            "|  3|Valle del Cauca| Femenino|Bachillerato|\n",
            "|  3|Valle del Cauca| Femenino|Bachillerato|\n",
            "+---+---------------+---------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analizar las 5 V del Big Data"
      ],
      "metadata": {
        "id": "FCeuHRADUb2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volumen\n",
        "\n",
        "El volumen hace referencia a la cantidad de datos que una organización debe manejar, almacenar o analizar. A medida que el volumen crece, los métodos tradicionales (como Excel) se vuelven insuficientes, y herramientas distribuidas como PySpark resultan más eficientes.\n",
        "\n",
        "Ejemplo de análisis:\n",
        "Ver cuántos registros (filas) y cuántas variables (columnas) tiene el dataset."
      ],
      "metadata": {
        "id": "ztZd3ynTUhPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar filas y columnas\n",
        "num_filas = df.count()\n",
        "num_columnas = len(df.columns)\n",
        "\n",
        "print(\"V de Volumen\")\n",
        "print(f\"El dataset contiene {int(num_filas):,} registros y {num_columnas} columnas.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG6H5A24UygY",
        "outputId": "cbd20cb4-9a56-4301-90bb-276a8b068cb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V de Volumen\n",
            "El dataset contiene 102 registros y 4 columnas.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver estructura y tipos de datos\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVFq4DydVcpM",
        "outputId": "23f0928f-dea7-4a68-83b4-bb29c54f7b95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Departamento: string (nullable = true)\n",
            " |-- Genero: string (nullable = true)\n",
            " |-- Grado: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar valores nulos (volumen de datos)\n",
        "nulos = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "print(\"\\n Conteo de valores nulos:\")\n",
        "nulos.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNWOHHWZXUZm",
        "outputId": "b01f35e3-e624-40c6-809f-2c69fe13bb94"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Conteo de valores nulos:\n",
            "+---+------------+------+-----+\n",
            "| ID|Departamento|Genero|Grado|\n",
            "+---+------------+------+-----+\n",
            "|  0|           0|     2|    1|\n",
            "+---+------------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar ejemplo de datos\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9mJ_VDVXbVQ",
        "outputId": "055959d4-a490-4538-aca6-c22eb0dc398f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+---------+------------+\n",
            "| ID|   Departamento|   Genero|       Grado|\n",
            "+---+---------------+---------+------------+\n",
            "|  1|      Santander|Masculino|    Primaria|\n",
            "|  2|      Santander|        F|  Secundaria|\n",
            "|  3|Valle del Cauca| Femenino|Bachillerato|\n",
            "|  3|Valle del Cauca| Femenino|Bachillerato|\n",
            "|  3|Valle del Cauca| Femenino|Bachillerato|\n",
            "+---+---------------+---------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación\n",
        "\n",
        "* Mide cuánto se puede procesar con Spark.\n",
        "* Los valores nulos indican volumen no aprovechable.\n",
        "* PySpark puede procesar millones de filas distribuidas en memoria y disco sin problemas de rendimiento."
      ],
      "metadata": {
        "id": "gq30GbBtXgwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Velocidad\n",
        "\n",
        "La velocidad mide la rapidez con la que los datos se generan, procesan o actualizan. En Big Data, la información llega en tiempo real o en grandes lotes continuos (por ejemplo, sensores IoT, transacciones financieras o logs web).\n",
        "\n",
        "Ejemplo de análisis:\n",
        "Medir el tiempo que tarda PySpark en ejecutar una operación, como calcular un promedio."
      ],
      "metadata": {
        "id": "-6v-Xe3FXt8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print(\" V de Velocidad\")\n",
        "\n",
        "# Medir tiempo de carga y procesamiento\n",
        "inicio_conteo = time.time()\n",
        "total_registros = df.count()\n",
        "fin_conteo = time.time()\n",
        "\n",
        "print(f\" Tiempo de conteo de registros: {fin_conteo - inicio_conteo:.4f} segundos\")\n",
        "print(f\" Total de registros procesados: {total_registros}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSDp0NtpXgkK",
        "outputId": "a70f201d-ccef-4cd7-db84-8286fb837154"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " V de Velocidad\n",
            " Tiempo de conteo de registros: 0.4062 segundos\n",
            " Total de registros procesados: 102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Medir tiempo de una agregación simple: contar personas por Departamento\n",
        "inicio_agg = time.time()\n",
        "df.groupBy(\"Departamento\").agg(F.count(\"ID\").alias(\"Total_por_Departamento\")).show(5)\n",
        "fin_agg = time.time()\n",
        "\n",
        "print(f\" Tiempo de agregación por Departamento: {fin_agg - inicio_agg:.4f} segundos\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A1jW1z4YyBA",
        "outputId": "fda44cc5-c4b4-4ac1-f414-35529ebb8df0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------------------+\n",
            "|   Departamento|Total_por_Departamento|\n",
            "+---------------+----------------------+\n",
            "|   Cundinamarca|                    11|\n",
            "|      Antioquia|                    13|\n",
            "|      Santander|                    20|\n",
            "|      Atlántico|                     8|\n",
            "|Valle del Cauca|                    18|\n",
            "+---------------+----------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            " Tiempo de agregación por Departamento: 0.7417 segundos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_duplicados_por_id(dataframe):\n",
        "    \"\"\"\n",
        "    Elimina filas duplicadas basadas en la columna 'ID'.\n",
        "    Conserva solo el primer registro de cada ID.\n",
        "    \"\"\"\n",
        "    df_sin_duplicados = dataframe.dropDuplicates([\"ID\"])\n",
        "    return df_sin_duplicados\n",
        "\n",
        "# Aplicamos la función\n",
        "df_limpio = eliminar_duplicados_por_id(df)\n",
        "print(\" Duplicados eliminados por ID.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtljEf9OdpGZ",
        "outputId": "9ca93136-e9c3-42d0-9b47-6ca1b3160ba5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Duplicados eliminados por ID.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Genero\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XznE0q8d3J8",
        "outputId": "e92cad6a-d73b-4f8e-8fd6-44f03d0cca5c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|   Genero|count|\n",
            "+---------+-----+\n",
            "|       Fe|   16|\n",
            "|        F|   19|\n",
            "|     NULL|    2|\n",
            "|        M|   18|\n",
            "| Femenino|   24|\n",
            "|Masculino|   23|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estandarizar_genero(texto):\n",
        "    \"\"\"\n",
        "    Corrige las variantes comunes del género:\n",
        "    F, FEM, FEMENINA → FEMENINO\n",
        "    M, MASC, MASCULINA → MASCULINO\n",
        "    \"\"\"\n",
        "    if texto is None:\n",
        "        return None\n",
        "\n",
        "    texto = texto.upper().strip()\n",
        "    if texto in [\"F\", \"FE\", \"FEM\", \"FEMENINA\", \"FEMENINO\"]:\n",
        "        return \"FEMENINO\"\n",
        "    elif texto in [\"M\", \"MA\", \"MASC\", \"MASCULINA\", \"MASCULINO\"]:\n",
        "        return \"MASCULINO\"\n",
        "    else:\n",
        "        return texto\n",
        "\n",
        "# Registrar UDF\n",
        "udf_estandarizar_genero = F.udf(estandarizar_genero, StringType())\n",
        "\n",
        "# Aplicar a la columna de género\n",
        "df_limpio = df_limpio.withColumn(\"Genero\", udf_estandarizar_genero(F.col(\"Genero\")))\n"
      ],
      "metadata": {
        "id": "B2ADYbUHeTxU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_limpio.groupBy(\"Genero\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuYOqzqceudN",
        "outputId": "cab0a541-c0bc-4298-aa9d-790aca091c4e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|   Genero|count|\n",
            "+---------+-----+\n",
            "| FEMENINO|   57|\n",
            "|     NULL|    2|\n",
            "|MASCULINO|   41|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sin_nulls = df_limpio.na.drop()"
      ],
      "metadata": {
        "id": "mMDgg64TfAXD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sin_nulls.groupBy(\"Genero\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi_QNkkFfCds",
        "outputId": "3e4ebda7-2fda-431f-e76a-78e95db18f9e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|   Genero|count|\n",
            "+---------+-----+\n",
            "| FEMENINO|   56|\n",
            "|MASCULINO|   41|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación\n",
        "\n",
        "* La velocidad se mide por el tiempo que tarda Spark en ejecutar tareas comunes: conteo, agregación, filtrado, etc.\n",
        "* Incluso con datasets grandes, PySpark mantiene tiempos bajos gracias al procesamiento en memoria y en paralelo.\n",
        "* Si repites el código varias veces, notarás que Spark cachea resultados, reduciendo el tiempo en ejecuciones posteriores."
      ],
      "metadata": {
        "id": "_uXhtTEDZz8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variedad\n",
        "\n",
        "La variedad representa los diferentes tipos y formatos de datos que se manejan:\n",
        "estructurados (tablas), semiestructurados (JSON, XML) y no estructurados (texto, imágenes, audio, video). En PySpark, la variedad se aborda con diferentes APIs (Spark SQL, Spark Streaming, MLlib, etc.).\n",
        "\n",
        "Ejemplo de análisis:\n",
        "Identificar los tipos de datos en el dataset (numéricos, texto, categóricos etc)."
      ],
      "metadata": {
        "id": "s0ta8PjLX1Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" V de Variedad\")\n",
        "\n",
        "# Ver tipos de datos\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In2GIhjRX_tZ",
        "outputId": "baee73b4-b19a-4766-b50d-d64893938e6e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " V de Variedad\n",
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Departamento: string (nullable = true)\n",
            " |-- Genero: string (nullable = true)\n",
            " |-- Grado: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar columnas categóricas y numéricas\n",
        "categoricas = [c for (c, t) in df.dtypes if t == \"string\"]\n",
        "numericas = [c for (c, t) in df.dtypes if t in [\"int\", \"double\", \"float\"]]\n",
        "\n",
        "print(f\"Columnas categóricas: {categoricas}\")\n",
        "print(f\"Columnas numéricas: {numericas}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2ja6kYlaIR6",
        "outputId": "a5e3a48d-ccf0-4091-eef6-4680bc1c8fe8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas categóricas: ['Departamento', 'Genero', 'Grado']\n",
            "Columnas numéricas: ['ID']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analizar distribución de una variable categórica\n",
        "df.groupBy(\"GENERO\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBysTHpnaK_W",
        "outputId": "936490fa-4d46-494f-f36d-1902c416081b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|   GENERO|count|\n",
            "+---------+-----+\n",
            "|       Fe|   16|\n",
            "|        F|   19|\n",
            "|     NULL|    2|\n",
            "|        M|   18|\n",
            "| Femenino|   24|\n",
            "|Masculino|   23|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación\n",
        "\n",
        "* Spark maneja datos heterogéneos fácilmente dentro del mismo DataFrame.\n",
        "* Permite trabajar con texto, números o fechas sin conversiones manuales.\n",
        "* Esta flexibilidad facilita la integración de fuentes diversas (por ejemplo, IoT, CSV, logs)."
      ],
      "metadata": {
        "id": "UngH3jqOaZVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veracidad\n",
        "\n",
        "La veracidad mide la calidad, confiabilidad y consistencia de los datos.\n",
        "En Big Data, los errores, duplicados y valores nulos son frecuentes, y deben detectarse y corregirse para asegurar resultados válidos.\n",
        "\n",
        "Ejemplo de análisis:\n",
        "Contar valores duplicados en el dataset."
      ],
      "metadata": {
        "id": "3nxSsdHNX_2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" V de Veracidad\")\n",
        "\n",
        "# Contar duplicados\n",
        "duplicados = df.count() - df.dropDuplicates().count()\n",
        "print(f\"Número de registros duplicados: {duplicados}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFwGTFVRYHeZ",
        "outputId": "aefbc502-5819-450e-f44a-d5f3ae9b0b20"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " V de Veracidad\n",
            "Número de registros duplicados: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores inconsistentes (ej. diferentes formas de escribir el mismo valor)\n",
        "df.select(\"Departamento\").groupBy(\"Departamento\").count().orderBy(F.desc(\"count\")).show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFqFoVByannG",
        "outputId": "8776bcd3-fb0c-47ae-d6d0-6eb4ca0f64d9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----+\n",
            "|   Departamento|count|\n",
            "+---------------+-----+\n",
            "|      Santander|   20|\n",
            "|Valle del Cauca|   18|\n",
            "|         Bogotá|   18|\n",
            "|      Bogotá DC|   14|\n",
            "|      Antioquia|   13|\n",
            "|   Cundinamarca|   11|\n",
            "|      Atlántico|    8|\n",
            "+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación\n",
        "\n",
        "* Spark facilita la limpieza y estandarización de datos a gran escala.\n",
        "* Permite detectar y corregir inconsistencias sin afectar el rendimiento.\n",
        "* La veracidad garantiza que los análisis posteriores sean confiables."
      ],
      "metadata": {
        "id": "J9RaYBZNb9iV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valor\n",
        "\n",
        "El valor se refiere a la utilidad real de los datos para generar conocimiento o tomar decisiones. Un conjunto de datos tiene valor cuando permite extraer patrones, optimizar procesos o apoyar políticas basadas en evidencia.\n",
        "\n",
        "Ejemplo de análisis:\n",
        "Obtener información útil como promedios, distribuciones o tendencias."
      ],
      "metadata": {
        "id": "gsaGvTdmYHnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ANÁLISIS DE VALOR\")\n",
        "\n",
        "# Agrupar por la columna Genero y contar cuántos registros hay por cada categoría\n",
        "conteo_genero = (\n",
        "    df_sin_nulls.groupBy(\"Genero\")\n",
        "    .count()\n",
        "    .orderBy(F.desc(\"count\"))\n",
        ")\n",
        "\n",
        "# Mostrar la tabla resumen\n",
        "conteo_genero.show()\n",
        "\n",
        "# Calcular el total de registros para obtener proporciones\n",
        "total = df_sin_nulls.count()\n",
        "\n",
        "# Agregar una nueva columna con el porcentaje que representa cada género\n",
        "df_valor = conteo_genero.withColumn(\n",
        "    \"Porcentaje\",\n",
        "    (F.col(\"count\") / total * 100).cast(\"double\")\n",
        ")\n",
        "\n",
        "# Mostrar resultados finales\n",
        "df_valor.show()\n",
        "\n",
        "# Mostrar los resultados con una interpretación\n",
        "print(\"Interpretación:\")\n",
        "print(\"Este análisis nos muestra la distribución de estudiantes por género.\")\n",
        "print(\"Con esta información podemos evaluar el equilibrio en los datos y detectar posibles sesgos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEUO5nHcfcIc",
        "outputId": "35ccfd7d-3a2a-49fc-855d-d030d96c8f17"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANÁLISIS DE VALOR\n",
            "+---------+-----+\n",
            "|   Genero|count|\n",
            "+---------+-----+\n",
            "| FEMENINO|   56|\n",
            "|MASCULINO|   41|\n",
            "+---------+-----+\n",
            "\n",
            "+---------+-----+-----------------+\n",
            "|   Genero|count|       Porcentaje|\n",
            "+---------+-----+-----------------+\n",
            "| FEMENINO|   56|57.73195876288659|\n",
            "|MASCULINO|   41| 42.2680412371134|\n",
            "+---------+-----+-----------------+\n",
            "\n",
            "Interpretación:\n",
            "Este análisis nos muestra la distribución de estudiantes por género.\n",
            "Con esta información podemos evaluar el equilibrio en los datos y detectar posibles sesgos.\n"
          ]
        }
      ]
    }
  ]
}